{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering, Preprocessing, and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "bgg = pd.read_csv('browse_cmf_credits_raw.csv', index_col=0)\n",
    "pd.set_option('max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19019 entries, 0 to 19032\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   avg_rating     19019 non-null  float64\n",
      " 1   geek_rating    19019 non-null  float64\n",
      " 2   num_voters     19019 non-null  int64  \n",
      " 3   title          19019 non-null  object \n",
      " 4   full_game_url  19019 non-null  object \n",
      " 5   rank           19019 non-null  int64  \n",
      " 6   game_id        19019 non-null  int64  \n",
      " 7   category       18808 non-null  object \n",
      " 8   mechanic       17452 non-null  object \n",
      " 9   family         14405 non-null  object \n",
      " 10  age            19019 non-null  int64  \n",
      " 11  max_play_time  19019 non-null  int64  \n",
      " 12  max_players    19019 non-null  int64  \n",
      " 13  min_play_time  19019 non-null  int64  \n",
      " 14  min_players    19019 non-null  int64  \n",
      " 15  weight         19019 non-null  float64\n",
      "dtypes: float64(3), int64(8), object(5)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "bgg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rating       float64\n",
       "geek_rating      float64\n",
       "num_voters         int64\n",
       "title             object\n",
       "full_game_url     object\n",
       "rank               int64\n",
       "game_id            int64\n",
       "category          object\n",
       "mechanic          object\n",
       "family            object\n",
       "age                int64\n",
       "max_play_time      int64\n",
       "max_players        int64\n",
       "min_play_time      int64\n",
       "min_players        int64\n",
       "weight           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the data types are correct\n",
    "bgg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rating          0\n",
       "geek_rating         0\n",
       "num_voters          0\n",
       "title               0\n",
       "full_game_url       0\n",
       "rank                0\n",
       "game_id             0\n",
       "category          211\n",
       "mechanic         1567\n",
       "family           4614\n",
       "age                 0\n",
       "max_play_time       0\n",
       "max_players         0\n",
       "min_play_time       0\n",
       "min_players         0\n",
       "weight              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "bgg.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "mechanic    0\n",
       "family      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling nulls in categorical labels with \"None\"\n",
    "bgg.fillna('None', inplace=True)\n",
    "\n",
    "# confirming categorical nulls are replaced\n",
    "bgg[['category','mechanic','family']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The family label is sparse with 2,748 different families. Using this as a feature would increase dimensionality significantly, but also many of families are unique or specific to a game or set of games, so this will not generalize well to new data. However, as we saw in EDA, Kickstarter games have a statistically higher average Geek Rating, so I do want to create a column indicating if a game was or wasn't on Kickstarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column named Kickstarter and drop family column\n",
    "bgg['kickstarter'] = bgg['family'].str.contains('Kickstarter').replace({True:1,False:0})\n",
    "bgg.drop(columns=['family'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy columns for category label\n",
    "cat_dummy = bgg.category.str.get_dummies(',').add_prefix('cat_')\n",
    "\n",
    "# creating dummy columns for mechanic label\n",
    "# three mechanics have commas which need to be removed before creating dummies \n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('Deck, Bag, and Pool Building', 'Deck Bag and Pool Building')\n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('I Cut, You Choose', 'I Cut You Choose')\n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('Worker Placement, Different Worker Types', 'Worker Placement Different Worker Types')\n",
    "mech_dummy = bgg.mechanic.str.get_dummies(',').add_prefix('mech_')\n",
    "\n",
    "# concatenating with original dataframe and dropping the category and mechanic columns\n",
    "bgg = pd.concat([bgg, cat_dummy, mech_dummy], axis=1)\n",
    "bgg.drop(columns=['category','mechanic'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split, Scaling and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable will be Geek Rating, specifically if a game be in the top 1,000. Since its derived from average rating we will drop average rating. Number of voters correlates with Geek Rating, but in this context I want the model to predict the rating of a new game. Any value for number of voters for a new game would be arbitrary. Title, game url, rank, and game id are not relevant to the model and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating binary response column for top 1000 games\n",
    "bgg['top_1000'] = bgg['rank'] <= 1000\n",
    "bgg['top_1000'].replace({True:1,False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting features, X and response variable, y\n",
    "X = bgg.drop(columns=['avg_rating','geek_rating','num_voters','title','full_game_url','rank','game_id','top_1000'])\n",
    "y = bgg.top_1000\n",
    "\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting numerical values from the bgg data set\n",
    "X = bgg[['age', 'max_play_time','max_players','min_play_time','min_players','weight']]\n",
    "imputer = KNNImputer(missing_values=0)\n",
    "\n",
    "# create impute DF and merge with text values\n",
    "bgg_impute = pd.DataFrame(imputer.fit_transform(X), columns=['age', 'max_play_time','max_players','min_play_time','min_players','weight'])\n",
    "bgg_to_append = bgg[['avg_rating','geek_rating','num_voters','title','full_game_url','rank','game_id','category','mechanic','family']]\n",
    "bgg_impute = pd.merge(bgg_to_append, bgg_impute, on=bgg_to_append.index)\n",
    "\n",
    "# rounding \n",
    "bgg.age = bgg.age.round()\n",
    "bgg.min_players = bgg.min_players.round()\n",
    "bgg.max_players = bgg.max_players.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling - fit on train, transform both train and teset\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_train)\n",
    "X_train = standard_scaler.transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determening the number of components to keep in principal component analysis\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# function to determine the number of components needed to reach desired explained variance\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    total_variance = 0.0\n",
    "    n_components = 0\n",
    "    \n",
    "    # for the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # break if we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "goal_var = 0.95\n",
    "components = select_n_components(explained_variance,goal_var)\n",
    "print(f'{components} components are needed to explain {goal_var}% of the variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting PCA on train and transforming both train and test\n",
    "pca = PCA(n_components=components)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying linear discriminant analysis: this returns 1 component since max components = num classes - 1 \n",
    "# and there are only 2 classes in this data set\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "X_train = lda.transform(X_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Trying some out of the box models to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,logreg_pred))\n",
    "print(\"Balanced Accuracy:\", metrics.balanced_accuracy_score(y_test,logreg_pred))\n",
    "print('Precision Score:', metrics.precision_score(y_test,logreg_pred, pos_label = 1))\n",
    "print('Recall Score:', metrics.recall_score(y_test,logreg_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=23)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "leaves = tree.get_n_leaves()\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,tree_pred))\n",
    "print('Balanced Accuracy:', metrics.balanced_accuracy_score(y_test,tree_pred))\n",
    "print('Precision Score:' , metrics.precision_score(y_test,tree_pred, pos_label = 1))\n",
    "print('Recall Score:', metrics.recall_score(y_test,tree_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=23, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,rf_pred))\n",
    "print('Balanced Accuracy:', metrics.balanced_accuracy_score(y_test,rf_pred))\n",
    "print('Precision Score:' , metrics.precision_score(y_test,rf_pred, pos_label = 1))\n",
    "print('Recall Score:' , metrics.recall_score(y_test,rf_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuray and Recall are high, but Precision is bad across the board. This makes sense as the number of games in the top 1000 is only about 5% of the dataset and this is reflected in Balanced Accuracy. Basically the models are predicting too many games to be in the top 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Next Steps\n",
    "\n",
    "Do PCA on related features\n",
    "Use ROC/AUC score because - threshold independent (default is 0.5)\n",
    "model outputs are a probaility so can adjust threshold to be more confident in positives\n",
    "\n",
    "ROC AUC for figuring out which model is best \n",
    "then use gridsearch to determine threshold \n",
    "\n",
    "use classification report\n",
    "\n",
    "Use:\n",
    "- pipelines\n",
    "- gridsearchcv\n",
    "\n",
    "Try:\n",
    "- to see if I can keep the intepretability\n",
    "- binary encoding? Less interpretability, but maybe more accuracy\n",
    "- limiting label volume with value counts\n",
    "- remove outliers outside 3 stds\n",
    "- bucket ages - look at how the buckets relate to the target variable\n",
    "\n",
    "How to increase precision?\n",
    "- Try more models\n",
    "- Oversample\n",
    "- Synthetic samples with - Synthetic Minority Oversampling Technique (SMOAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
