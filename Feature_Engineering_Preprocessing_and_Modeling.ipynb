{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering, Preprocessing, and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "bgg = pd.read_csv('bgg_clean_impute.csv', index_col=0)\n",
    "pd.set_option('max_colwidth', 120)\n",
    "\n",
    "# KNN was used to impute missing values. I'm rounding these columns to remove inconsistencies with age which \n",
    "# is recorded as a whole number as well as with player count which must be a whole number.\n",
    "bgg.age = bgg.age.round()\n",
    "bgg.min_players = bgg.min_players.round()\n",
    "bgg.max_players = bgg.max_players.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rating       float64\n",
       "geek_rating      float64\n",
       "num_voters         int64\n",
       "title             object\n",
       "full_game_url     object\n",
       "rank               int64\n",
       "game_id            int64\n",
       "category          object\n",
       "mechanic          object\n",
       "family            object\n",
       "age              float64\n",
       "max_play_time    float64\n",
       "max_players      float64\n",
       "min_play_time    float64\n",
       "min_players      float64\n",
       "weight           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the data types are correct\n",
    "bgg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rating          0\n",
       "geek_rating         0\n",
       "num_voters          0\n",
       "title               0\n",
       "full_game_url       0\n",
       "rank                0\n",
       "game_id             0\n",
       "category          211\n",
       "mechanic         1567\n",
       "family           4614\n",
       "age                 0\n",
       "max_play_time       0\n",
       "max_players         0\n",
       "min_play_time       0\n",
       "min_players         0\n",
       "weight              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "bgg.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "mechanic    0\n",
       "family      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling nulls in categorical labels with \"None\"\n",
    "bgg.fillna('None', inplace=True)\n",
    "\n",
    "# confirming categorical nulls are replaced\n",
    "bgg[['category','mechanic','family']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The family label is sparse with 2,748 different families. Using this as a feature would increase dimensionality significantly, but also many of families are unique or specific to a game or set of games, so this will not generalize well to new data. However, as we saw in EDA, Kickstarter games have a statistically higher average Geek Rating, so I do want to create a column indicating if a game was or wasn't on Kickstarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column named Kickstarter and drop family column\n",
    "bgg['kickstarter'] = bgg['family'].str.contains('Kickstarter').replace({True:1,False:0})\n",
    "bgg.drop(columns=['family'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy columns for category label\n",
    "cat_dummy = bgg.category.str.get_dummies(',').add_prefix('cat_')\n",
    "\n",
    "# creating dummy columns for mechanic label\n",
    "# three mechanics have commas which need to be removed before creating dummies \n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('Deck, Bag, and Pool Building', 'Deck Bag and Pool Building')\n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('I Cut, You Choose', 'I Cut You Choose')\n",
    "bgg['mechanic'] = bgg['mechanic'].str.replace('Worker Placement, Different Worker Types', 'Worker Placement Different Worker Types')\n",
    "mech_dummy = bgg.mechanic.str.get_dummies(',').add_prefix('mech_')\n",
    "\n",
    "# concatenating with original dataframe and dropping the category and mechanic columns\n",
    "bgg = pd.concat([bgg, cat_dummy, mech_dummy], axis=1)\n",
    "bgg.drop(columns=['category','mechanic'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split, Scaling and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable will be Geek Rating, specifically if a game be in the top 1,000. Since its derived from average rating we will drop average rating. Number of voters correlates with Geek Rating, but in this context I want the model to predict the rating of a new game. Any value for number of voters for a new game would be arbitrary. Title, game url, rank, and game id are not relevant to the model and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating binary response column for top 1000 games\n",
    "bgg['top_1000'] = bgg['rank'] <= 1000\n",
    "bgg['top_1000'].replace({True:1,False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting features, X and response variable, y\n",
    "X = bgg.drop(columns=['avg_rating','geek_rating','num_voters','title','full_game_url','rank','game_id','top_1000'])\n",
    "y = bgg.top_1000\n",
    "\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling - fit on train, transform both train and teset\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_train)\n",
    "X_train = standard_scaler.transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 components are needed to explain 0.95% of the variance\n"
     ]
    }
   ],
   "source": [
    "# determening the number of components to keep in principal component analysis\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# function to determine the number of components needed to reach desired explained variance\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    total_variance = 0.0\n",
    "    n_components = 0\n",
    "    \n",
    "    # for the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # break if we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "goal_var = 0.95\n",
    "components = select_n_components(explained_variance,goal_var)\n",
    "print(f'{components} components are needed to explain {goal_var}% of the variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting PCA on train and transforming both train and test\n",
    "pca = PCA(n_components=components)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying linear discriminant analysis: this returns 1 component since max components = num classes - 1 \n",
    "# and there are only 2 classes in this data set\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "X_train = lda.transform(X_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Trying some out of the box models to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.9471608832807571\n",
      "Balanced Accuracy: 0.5990882205324425\n",
      "Precision Score: 0.5308641975308642\n",
      "Recall Score: 0.9894385769872152\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,logreg_pred))\n",
    "print(\"Balanced Accuracy:\", metrics.balanced_accuracy_score(y_test,logreg_pred))\n",
    "print('Precision Score:', metrics.precision_score(y_test,logreg_pred, pos_label = 1))\n",
    "print('Recall Score:', metrics.recall_score(y_test,logreg_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.9216614090431126\n",
      "Balanced Accuracy: 0.6245082219355953\n",
      "Precision Score: 0.2830188679245283\n",
      "Recall Score: 0.9577543079488605\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=23)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "leaves = tree.get_n_leaves()\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,tree_pred))\n",
    "print('Balanced Accuracy:', metrics.balanced_accuracy_score(y_test,tree_pred))\n",
    "print('Precision Score:' , metrics.precision_score(y_test,tree_pred, pos_label = 1))\n",
    "print('Recall Score:', metrics.recall_score(y_test,tree_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.9458464773922187\n",
      "Balanced Accuracy: 0.5709347695861239\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.9913841022790439\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=23, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print('Accuray:', metrics.accuracy_score(y_test,rf_pred))\n",
    "print('Balanced Accuracy:', metrics.balanced_accuracy_score(y_test,rf_pred))\n",
    "print('Precision Score:' , metrics.precision_score(y_test,rf_pred, pos_label = 1))\n",
    "print('Recall Score:' , metrics.recall_score(y_test,rf_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuray and Recall are high, but Precision is bad across the board. This makes sense as the number of games in the top 1000 is only about 5% of the dataset and this is reflected in Balanced Accuracy. Basically the models are predicting too many games to be in the top 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Next Steps\n",
    "\n",
    "Use:\n",
    "- pipelines\n",
    "- gridsearchcv\n",
    "\n",
    "Try:\n",
    "- to see if I can keep the intepretability\n",
    "- binary encoding? Less interpretability, but maybe more accuracy\n",
    "- limiting label volume with value counts\n",
    "- remove outliers outside 3 stds\n",
    "\n",
    "How to increase precision?\n",
    "- Try more models\n",
    "- Oversample\n",
    "- Synthetic samples with - Synthetic Minority Oversampling Technique (SMOAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
